{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gKl-_LFflE8J",
    "outputId": "bb08e83d-806b-43a3-9af3-d44010e7bbd0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "save_dir = \"/content/drive/MyDrive/f1_data\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "years = [2023, 2024, 2025]\n",
    "max_retries = 5\n",
    "base_delay = 5  # seconds if hit rate limit 429\n",
    "\n",
    "def safe_get_json(url, max_retries=5, base_delay=2):\n",
    "    \"\"\"Safe function to GET JSON with rate limit (429) handling.\"\"\"\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            res = requests.get(url)\n",
    "            if res.status_code == 429:\n",
    "                wait_time = base_delay * attempt\n",
    "                print(f\"429 Too Many Requests — wait {wait_time}s (attempt {attempt})\")\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "\n",
    "            data = res.json()\n",
    "            # if API returns error message in JSON\n",
    "            if isinstance(data, dict) and data.get(\"error\") == \"Too Many Requests\":\n",
    "                wait_time = base_delay * attempt\n",
    "                print(f\"Rate limit JSON — wait {wait_time}s (attempt {attempt})\")\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "\n",
    "            return data  # success\n",
    "        except Exception as e:\n",
    "            print(f\"    Error fetching {url}: {e}\")\n",
    "            if attempt < max_retries:\n",
    "                wait_time = base_delay * attempt\n",
    "                print(f\"Waiting {wait_time}s before retrying...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"Failed to fetch data after {max_retries} attempts.\")\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "for year in years:\n",
    "    print(f\"\\n Fetching Race data for year {year}...\\n\")\n",
    "    sessions = safe_get_json(f\"https://api.openf1.org/v1/sessions?year={year}&session_type=Race\")\n",
    "\n",
    "    if not isinstance(sessions, list):\n",
    "        print(f\"none valid Race sessions found for year {year}\")\n",
    "        continue\n",
    "\n",
    "    for session in sessions:\n",
    "        if not isinstance(session, dict):\n",
    "            continue\n",
    "        session_key = session.get(\"session_key\")\n",
    "        race_name = session.get(\"circuit_short_name\", \"Unknown\").replace(\" \", \"_\")\n",
    "        session_name = session.get(\"session_type\", \"Unknown\")\n",
    "        print(f\"Race: {race_name} ({session_key}) [{session_name}]\")\n",
    "\n",
    "        drivers = safe_get_json(f\"https://api.openf1.org/v1/drivers?session_key={session_key}\")\n",
    "\n",
    "        if not isinstance(drivers, list):\n",
    "            print(f\"drivers response is not valid: {drivers}\")\n",
    "            continue\n",
    "\n",
    "        for driver in drivers:\n",
    "            if not isinstance(driver, dict):\n",
    "                continue\n",
    "            driver_number = driver.get(\"driver_number\")\n",
    "            driver_name = driver.get(\"full_name\", \"Unknown\").replace(\" \", \"_\")\n",
    "            if not driver_number:\n",
    "                continue\n",
    "\n",
    "            filename = f\"{year}_{race_name}_{session_name}_{session_key}_{driver_number}.csv\"\n",
    "            filepath = os.path.join(save_dir, filename)\n",
    "\n",
    "            if os.path.exists(filepath):\n",
    "                print(f\"Skipping, already exists: {filename}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"fetching car data for {driver_name} (#{driver_number})...\")\n",
    "\n",
    "            url_car = (\n",
    "                f\"https://api.openf1.org/v1/car_data?\"\n",
    "                f\"session_key={session_key}&driver_number={driver_number}&speed>=100\"\n",
    "            )\n",
    "\n",
    "            car_data = safe_get_json(url_car, max_retries=max_retries, base_delay=base_delay)\n",
    "            if not isinstance(car_data, list) or not car_data:\n",
    "                print(f\"no car data found for {driver_name}\")\n",
    "                continue\n",
    "\n",
    "            df = pd.DataFrame(car_data)\n",
    "            df[\"year\"] = year\n",
    "            df[\"race\"] = race_name\n",
    "            df[\"driver_number\"] = driver_number\n",
    "            df[\"driver_name\"] = driver_name\n",
    "            df.to_csv(filepath, index=False)\n",
    "            print(f\"data {driver_name} saved ({len(df)} rows)\")\n",
    "\n",
    "            time.sleep(0.5)\n",
    "\n",
    "print(f\"file saved to: {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "id": "a971a05d",
    "outputId": "0f0e2d0d-1ad9-4c04-abfa-395db607bc3d"
   },
   "outputs": [],
   "source": [
    "# Read the sample CSV into a new DataFrame\n",
    "df_sample_read = pd.read_csv(\"/content/drive/MyDrive/f1_data/2023_Sakhir_Race_7953_1.csv\")\n",
    "\n",
    "# Display the head of the new DataFrame\n",
    "display(df_sample_read.head())\n",
    "display(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url_2023 = \"https://api.openf1.org/v1/sessions?year=2023&session_type=Race\"\n",
    "url_2024 = \"https://api.openf1.org/v1/sessions?year=2024&session_type=Race\"\n",
    "url_2025 = \"https://api.openf1.org/v1/sessions?year=2025&session_type=Race\"\n",
    "df_2023 = pd.read_json(url_2023)\n",
    "df_2024 = pd.read_json(url_2024)\n",
    "df_2025 = pd.read_json(url_2025)\n",
    "df_all = pd.concat([df_2023, df_2024, df_2025], ignore_index=True)\n",
    "df_all.to_csv(\"all_race_sessions_2023_2025.csv\", index=False)\n",
    "print(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Path setup (relative to script location)\n",
    "script_dir = os.getcwd()\n",
    "raw_dir = os.path.join(script_dir, \"f1_data\")\n",
    "race_info_path = os.path.join(script_dir, \"data_scraping\", \"f1_races_2023_2025.csv\")\n",
    "output_dir = os.path.join(script_dir, \"f1_cleaned_data\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# load race info\n",
    "race_info = pd.read_csv(race_info_path)\n",
    "race_map = dict(zip(race_info[\"session_key\"], race_info[\"session_name\"]))  # map session_key → session_name\n",
    "\n",
    "# cleaning rules\n",
    "numeric_columns = [\"speed\", \"throttle\", \"rpm\", \"n_gear\"]\n",
    "bounds = {\n",
    "    \"speed\": (0, 400),\n",
    "    \"throttle\": (0, 100),\n",
    "    \"rpm\": (0, 20000),\n",
    "    \"n_gear\": (-1, 8)\n",
    "}\n",
    "\n",
    "# process all files\n",
    "csv_files = glob.glob(os.path.join(raw_dir, \"*.csv\"))\n",
    "print(f\"{len(csv_files)} files found.\")\n",
    "\n",
    "for file_path in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        file_name = os.path.basename(file_path)\n",
    "\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        for col in numeric_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "                low, high = bounds[col]\n",
    "                df = df[(df[col] >= low) & (df[col] <= high)]\n",
    "\n",
    "        df.dropna(subset=[\"speed\", \"throttle\", \"rpm\", \"n_gear\"], inplace=True)\n",
    "\n",
    "        # identify session type and year\n",
    "        session_key = str(df[\"session_key\"].iloc[0])\n",
    "        session_name = race_map.get(int(session_key), \"Unknown\")\n",
    "\n",
    "        year = str(df[\"year\"].iloc[0])\n",
    "        save_dir = os.path.join(output_dir, year, session_name.lower())\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        save_path = os.path.join(save_dir, file_name)\n",
    "        df.to_csv(save_path, index=False)\n",
    "        print(f\" done {file_name} → {session_name} ({len(df)} rows)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"error {os.path.basename(file_path)}: {e}\")\n",
    "\n",
    "print(f\"Output saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "script_dir = os.getcwd()\n",
    "base_dir = os.path.join(script_dir, \"f1_cleaned_data\")\n",
    "output_dir = os.path.join(script_dir, \"f1_annual_data\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for year in [\"2023\", \"2024\", \"2025\"]:\n",
    "    for session_name in [\"race\", \"sprint\"]:\n",
    "        input_path = os.path.join(base_dir, year, session_name)\n",
    "        all_files = glob.glob(os.path.join(input_path, \"*.csv\"))\n",
    "        dfs = []\n",
    "        for file in all_files:\n",
    "            try:\n",
    "                df = pd.read_csv(file)\n",
    "                dfs.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\" failed {os.path.basename(file)}: {e}\")\n",
    "\n",
    "        if dfs:\n",
    "            df_merged = pd.concat(dfs, ignore_index=True)\n",
    "            save_path = os.path.join(output_dir, f\"f1_{year}_{session_name}.csv\")\n",
    "            df_merged.to_csv(save_path, index=False)\n",
    "            print(f\"{year} {session_name} done ({len(df_merged):,})\")\n",
    "        else:\n",
    "            print(f\"No files found for {year} {session_name}\")\n",
    "\n",
    "print(f\"Output saved to: {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
